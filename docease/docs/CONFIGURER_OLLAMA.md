# Guide : Configurer Ollama avec n8n

Guide complet pour utiliser Ollama (IA locale) avec votre workflow n8n.

## üìã Pr√©requis

- ‚úÖ Ollama install√© sur votre machine
- ‚úÖ Un mod√®le t√©l√©charg√© (ex: llama2, mistral, etc.)
- ‚úÖ Ollama en cours d'ex√©cution

## üöÄ Configuration Rapide

### √âtape 1 : V√©rifier qu'Ollama fonctionne

```powershell
# V√©rifier qu'Ollama r√©pond
curl http://localhost:11434/api/tags

# Si vous voyez la liste des mod√®les, Ollama fonctionne !
```

Ou testez dans votre navigateur : http://localhost:11434/api/tags

### √âtape 2 : T√©l√©charger un Mod√®le (si pas d√©j√† fait)

```powershell
# T√©l√©charger un mod√®le (remplacez "llama2" par celui que vous voulez)
ollama pull llama2

# Autres mod√®les populaires :
# ollama pull mistral
# ollama pull codellama
# ollama pull phi
```

### √âtape 3 : Tester Ollama Localement

```powershell
# Test simple
ollama run llama2 "Bonjour, comment allez-vous ?"
```

Si √ßa fonctionne, vous √™tes pr√™t !

---

## ‚öôÔ∏è Configuration dans n8n

Il y a **deux m√©thodes** pour utiliser Ollama avec n8n :

### M√©thode 1 : HTTP Request (Recommand√©e - Plus Simple)

Cette m√©thode est plus simple et ne n√©cessite pas de node communautaire suppl√©mentaire.

#### Configuration :

1. **Dans le workflow n8n**, trouvez le n≈ìud **"G√©n√©ration IA"**
2. **Remplacez-le** par un n≈ìud **"HTTP Request"**
3. **Configurez le n≈ìud** :
   - **Method** : `POST`
   - **URL** : `http://host.docker.internal:11434/api/generate`
   - **Authentication** : None
   - **Body Content Type** : JSON
   - **Body** :
   ```json
   {
     "model": "llama2",
     "prompt": "R√©digez un texte professionnel et courtois en fran√ßais de 2 √† 3 paragraphes bas√© sur le contexte suivant :\n\nContexte : {{ $json.contexte }}\n\nPoints importants : {{ $json.points_importants }}\n\nLe texte doit √™tre formel, professionnel et adapt√© √† une correspondance professionnelle. Incluez une introduction, un d√©veloppement des points mentionn√©s, et une conclusion ouverte.",
     "stream": false
   }
   ```
4. **Sauvegardez**

5. **Apr√®s le n≈ìud HTTP Request**, ajoutez un n≈ìud **"Set"** pour extraire la r√©ponse :
   - **Name** : `texte_ia`
   - **Value** : `={{ JSON.parse($json.body).response }}`

#### Note pour Windows/Docker :

**Important** : Pour acc√©der √† Ollama depuis le conteneur Docker sur Windows, utilisez :
- `http://host.docker.internal:11434` (au lieu de `localhost:11434`)

---

### M√©thode 2 : LangChain Node (Avanc√©e)

Si vous pr√©f√©rez utiliser le node LangChain (plus de contr√¥le mais plus complexe).

#### Installation du node LangChain :

1. Dans n8n ‚Üí **Settings** ‚Üí **Community Nodes**
2. Installez : `@n8n/n8n-nodes-langchain`

#### Configuration :

1. **Dans le workflow**, le n≈ìud "G√©n√©ration IA" utilise d√©j√† LangChain
2. **Modifiez le n≈ìud** pour utiliser Ollama :
   - **Model** : S√©lectionnez "Custom LLM"
   - **Base URL** : `http://host.docker.internal:11434`
   - **API Path** : `/api/generate`
   - **Model Name** : `llama2` (ou votre mod√®le)
   - **Temperature** : `0.7`
   - **Max Tokens** : `500`

---

## üîß Modifier le Workflow pour Ollama

Je vais cr√©er une version du workflow adapt√©e pour Ollama :

### Option A : Modifier le Workflow Import√©

1. Importez le workflow `generateur_document.json`
2. **Remplacez le n≈ìud "G√©n√©ration IA"** :
   - Supprimez-le ou d√©sactivez-le
   - Ajoutez un n≈ìud **HTTP Request** √† la place
   - Configurez comme indiqu√© ci-dessus

### Option B : Utiliser la Version Ollama du Workflow

Je peux cr√©er une version du workflow pr√©configur√©e pour Ollama. Dites-moi si vous voulez que je la cr√©e !

---

## üß™ Tester la Configuration

### Test 1 : V√©rifier la connexion depuis Docker

```powershell
# Depuis le conteneur Docker
docker exec -it n8n-local curl http://host.docker.internal:11434/api/tags
```

### Test 2 : Test depuis n8n

1. Cr√©ez un workflow de test simple :
   - **HTTP Request** ‚Üí **Set** (pour afficher la r√©ponse)
2. Configurez le HTTP Request avec Ollama
3. Ex√©cutez le workflow manuellement
4. V√©rifiez la r√©ponse dans le n≈ìud Set

### Test 3 : Test avec le Workflow Complet

1. Remplissez le formulaire avec des donn√©es de test
2. V√©rifiez les logs :
   ```powershell
   docker logs n8n-local -f
   ```
3. V√©rifiez que le texte est bien g√©n√©r√© dans le document final

---

## üêõ Probl√®mes Courants

### Ollama n'est pas accessible depuis Docker

**Sympt√¥me** : Erreur "Connection refused" ou timeout

**Solutions** :

1. **Sur Windows/Mac** : Utilisez `host.docker.internal` :
   ```
   http://host.docker.internal:11434
   ```

2. **Sur Linux** : Utilisez l'IP de l'h√¥te :
   ```powershell
   # Trouver l'IP de la machine h√¥te
   ip addr show docker0
   # Utilisez cette IP au lieu de localhost
   ```

3. **Alternative** : Exposez Ollama dans Docker (plus complexe)

### Le mod√®le n'est pas trouv√©

**Sympt√¥me** : Erreur "model not found"

**Solutions** :
- V√©rifiez que le mod√®le est bien t√©l√©charg√© : `ollama list`
- V√©rifiez que le nom du mod√®le dans n8n correspond exactement

### R√©ponse vide ou erreur

**Sympt√¥me** : Le texte n'est pas g√©n√©r√©

**Solutions** :
- V√©rifiez les logs Ollama (dans le terminal o√π Ollama tourne)
- V√©rifiez que `stream: false` dans la requ√™te HTTP
- V√©rifiez le format de la r√©ponse dans le n≈ìud Set

---

## üìù Exemple de Configuration Compl√®te HTTP Request

Voici la configuration exacte pour le n≈ìud HTTP Request :

### Configuration du N≈ìud HTTP Request

**Method** : `POST`  
**URL** : `http://host.docker.internal:11434/api/generate`  
**Body Content Type** : `JSON`

**Body** (JSON) :
```json
{
  "model": "llama2",
  "prompt": "R√©digez un texte professionnel et courtois en fran√ßais de 2 √† 3 paragraphes. Contexte : {{ $('Formater Donn√©es').item.json.contexte }}. Points importants : {{ $('Formater Donn√©es').item.json.points_importants }}. Le texte doit √™tre formel et professionnel.",
  "stream": false
}
```

### N≈ìud Set pour Extraire la R√©ponse

Apr√®s le HTTP Request, ajoutez un n≈ìud **Set** :

**Mode** : Manual  
**Values** :
- **Name** : `texte_ia`  
- **Value** : `={{ JSON.parse($json.body).response }}`

---

## üîÑ Workflow Modifi√© pour Ollama

Si vous voulez, je peux cr√©er une version modifi√©e du workflow `generateur_document.json` qui utilise Ollama directement.

Cette version aurait :
- ‚úÖ N≈ìud HTTP Request configur√© pour Ollama
- ‚úÖ Extraction de la r√©ponse automatique
- ‚úÖ Gestion des erreurs am√©lior√©e
- ‚úÖ Pr√™t √† l'emploi

Dites-moi si vous voulez que je la cr√©e !

---

## ‚úÖ Checklist de Configuration

- [ ] Ollama install√© et fonctionnel
- [ ] Mod√®le t√©l√©charg√© (test√© avec `ollama run`)
- [ ] Ollama accessible depuis Docker (`host.docker.internal:11434`)
- [ ] N≈ìud HTTP Request configur√© dans le workflow
- [ ] N≈ìud Set pour extraire la r√©ponse configur√©
- [ ] Test avec workflow de test r√©ussi
- [ ] Test avec workflow complet r√©ussi

---

## üí° Astuces

1. **Choisissez le bon mod√®le** :
   - `llama2` : Bon √©quilibre qualit√©/vitesse
   - `mistral` : Plus performant, plus rapide
   - `phi` : Tr√®s rapide, l√©ger

2. **Ajustez les param√®tres** :
   - `temperature` : Plus √©lev√© = plus cr√©atif (0.7-0.9)
   - `num_predict` : Nombre de tokens max (500 pour 2-3 paragraphes)

3. **Optimisez le prompt** :
   - Soyez sp√©cifique : "texte professionnel en fran√ßais"
   - Donnez le contexte : utilisez les variables du formulaire
   - Pr√©cisez le format attendu : "2 √† 3 paragraphes"

---

**Vous √™tes pr√™t √† utiliser Ollama avec n8n !** üöÄ

Si vous voulez, je peux adapter le workflow directement pour vous.

